{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea7ca7-849f-4052-8bd9-8a57a00306d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture Akhona Njeje.\n",
    "# Topic : NLP.\n",
    "\n",
    "# Step 1: Data Collection\n",
    "\n",
    "# The first step in any NLP project is data collection. For a sentiment analysis project, we might use a dataset of movie reviews such as the IMDB dataset,\n",
    "# which is widely used for this purpose. This dataset contains 50,000 reviews labeled as either positive or negative. \n",
    "# The choice of dataset is crucial because it determines the quality of the model.\n",
    "\n",
    "# Framework Used: Python’s requests library can be used to download datasets, or you could use pandas to read datasets directly from files. \n",
    "# If you're working with online datasets, datasets from the Hugging Face library is a good choice for accessing popular NLP datasets easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406e3a2-46ad-4d91-b024-2d766e2f442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "\n",
    "# Once you have your data, the next step is to preprocess it. This involves removing noise such as HTML tags, special characters, and stop words. \n",
    "# Tokenization (splitting sentences into words) and stemming/lemmatization (reducing words to their root form) are also part of preprocessing.\n",
    "\n",
    "# Framework Used: For text preprocessing, the nltk library or spaCy is often used. For example, you might use nltk for tokenization and \n",
    "# stop word removal, while spaCy can be used for more advanced preprocessing like lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Sample preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    text = [word for word in tokens if word not in stopwords.words('english')]  # Remove stop words\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc81425-8c24-4372-b9e8-b8c305819988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Extraction\n",
    "\n",
    "# After preprocessing, the text data needs to be converted into a numerical format that machine learning algorithms can understand. \n",
    "# This is where feature extraction comes in. Common techniques include Bag of Words (BoW), TF-IDF (Term Frequency-Inverse Document Frequency), \n",
    "# or word embeddings like Word2Vec or GloVe.\n",
    "\n",
    "# Framework Used: sklearn’s CountVectorizer and TfidfVectorizer can be used for BoW and TF-IDF respectively. For word embeddings, \n",
    "# gensim can be used to train Word2Vec models.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample feature extraction function\n",
    "def extract_features(corpus):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f88c91-22a0-44f5-86bc-67db9b6c3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Building\n",
    "\n",
    "# With features extracted, the next step is to build a machine learning model. For sentiment analysis, \n",
    "# you could use a variety of classifiers like Logistic Regression, Naive Bayes, or more advanced methods like Deep Learning models.\n",
    "\n",
    "# Framework Used: sklearn provides easy-to-use interfaces for logistic regression, Naive Bayes, and support vector machines. \n",
    "# For deep learning models, TensorFlow or PyTorch can be used.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample model building function\n",
    "def build_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3a9ae-0ba4-4629-b82b-ce9d5791f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Model Evaluation\n",
    "\n",
    "# After building the model, it’s essential to evaluate its performance using metrics like accuracy, precision, recall, and F1-score.\n",
    "# Framework Used: sklearn’s metrics module provides functions for calculating these metrics.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6626db-e457-4d64-88b5-8fb483ac8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Deployment\n",
    "\n",
    "# Finally, once you have a trained model, you may want to deploy it as an API. This involves saving the model and \n",
    "# creating a REST API using frameworks like Flask or FastAPI.\n",
    "\n",
    "# Framework Used: Flask or FastAPI for creating APIs, and joblib or pickle for saving models.\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('sentiment_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Summary\n",
    "# This project walks through the key steps in a basic NLP project from data collection to deployment. \n",
    "# Each step uses specific Python libraries chosen for their strengths in handling different tasks in the NLP pipeline. \n",
    "# This process not only demonstrates technical skill but also a methodological approach to problem-solving in NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
